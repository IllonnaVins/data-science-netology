{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ae0b15",
   "metadata": {},
   "source": [
    "# Домашнее задание к лекции «Введение в рекуррентные НС»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62743c29",
   "metadata": {},
   "source": [
    "## Neural Part Of Speech Tagging\n",
    "\n",
    "We're now going to solve the same problem of POS tagging with neural networks.\n",
    "<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n",
    "\n",
    "From deep learning perspective, this is a task of predicting a sequence of outputs aligned to a sequence of inputs. There are several problems that match this formulation:\n",
    "* Part Of Speech Tagging -  an auxuliary task for many NLP problems\n",
    "* Named Entity Recognition - for chat bots and web crawlers\n",
    "* Protein structure prediction - for bioinformatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45006989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cae7921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
    "\n",
    "data = np.array([ [(word.lower(),tag) for word,tag in sentence] for sentence in data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f271aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a09bd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>ADP</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>VERB</td><td>ADV</td><td>VERB</td><td>ADP</td><td>DET</td><td>ADJ</td><td>NOUN</td><td>.</td></tr><td>implementation</td><td>of</td><td>georgia's</td><td>automobile</td><td>title</td><td>law</td><td>was</td><td>also</td><td>recommended</td><td>by</td><td>the</td><td>outgoing</td><td>jury</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>PRON</td><td>VERB</td><td>ADP</td><td>DET</td><td>NOUN</td><td>.</td><td>VERB</td><td>NOUN</td><td>PRT</td><td>VERB</td><td>.</td><td>DET</td><td>NOUN</td><td>.</td></tr><td>it</td><td>urged</td><td>that</td><td>the</td><td>city</td><td>``</td><td>take</td><td>steps</td><td>to</td><td>remedy</td><td>''</td><td>this</td><td>problem</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>VERB</td></tr><td>merger</td><td>proposed</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "def draw(sentence):\n",
    "    words,tags = zip(*sentence)\n",
    "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
    "                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
    "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
    "    \n",
    "    \n",
    "draw(data[11])\n",
    "draw(data[10])\n",
    "draw(data[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229329d",
   "metadata": {},
   "source": [
    "### Building vocabularies\n",
    "\n",
    "Just like before, we have to build a mapping from tokens to integer ids. This time around, our model operates on a word level, processing one word per RNN step. This means we'll have to deal with far larger vocabulary.\n",
    "\n",
    "Luckily for us, we only receive those words as input i.e. we don't have to predict them. This means we can have a large vocabulary for free by using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581d8b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage = 0.92876\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "for sentence in data:\n",
    "    words,tags = zip(*sentence)\n",
    "    word_counts.update(words)\n",
    "\n",
    "all_words = ['#EOS#','#UNK#'] + list(list(zip(*word_counts.most_common(10000)))[0])\n",
    "\n",
    "#let's measure what fraction of data words are in the dictionary\n",
    "print(\"Coverage = %.5f\" % (float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8749db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = defaultdict(lambda:1, { word: i for i, word in enumerate(all_words) })\n",
    "tag_to_id = { tag: i for i, tag in enumerate(all_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33327a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(lines, token_to_id, max_len=None, pad=0, dtype='int32', time_major=False):\n",
    "    \"\"\"Converts a list of names into rnn-digestable matrix with paddings added after the end\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len,lines))\n",
    "    matrix = np.empty([len(lines), max_len],dtype)\n",
    "    matrix.fill(pad)\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = list(map(token_to_id.__getitem__,lines[i]))[:max_len]\n",
    "        matrix[i,:len(line_ix)] = line_ix\n",
    "\n",
    "    return matrix.T if time_major else matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab3a2128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ids:\n",
      "[[   2 3057    5    2 2238 1334 4238 2454    3    6   19   26 1070   69\n",
      "     8 2088    6    3    1    3  266   65  342    2    1    3    2  315\n",
      "     1    9   87  216 3322   69 1558    4    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  45   12    8  511 8419    6   60 3246   39    2    1    1    3    2\n",
      "   845    1    3    1    3   10 9910    2    1 3470    9   43    1    1\n",
      "     3    6    2 1046  385   73 4562    3    9    2    1    1 3250    3\n",
      "    12   10    2  861 5240   12    8 8936  121    1    4]\n",
      " [  33   64   26   12  445    7 7346    9    8 3337    3    1 2811    3\n",
      "     2  463  572    2    1    1 1649   12    1    4    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Tag ids:\n",
      "[[ 6  3  4  6  3  3  9  9  7 12  4  5  9  4  6  3 12  7  9  7  9  8  4  6\n",
      "   3  7  6 13  3  4  6  3  9  4  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 5  9  6  9  3 12  6  3  7  6 13  3  7  6 13  3  7 13  7  5  9  6  3  3\n",
      "   4  6 13  3  7 12  6  3  6 13  3  7  4  6  3  9  3  7  9  4  6 13  3  9\n",
      "   6  3  2 13  7]\n",
      " [ 4  6  5  9 13  4  3  4  6 13  7 13  3  7  6  3  4  6 13  3  3  9  9  7\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "batch_words, batch_tags = zip(*[zip(*sentence) for sentence in data[-3:]])\n",
    "\n",
    "print(\"Word ids:\")\n",
    "print(to_matrix(batch_words, word_to_id))\n",
    "print(\"Tag ids:\")\n",
    "print(to_matrix(batch_tags, tag_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aba703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(len(all_words),50))\n",
    "model.add(tf.keras.layers.SimpleRNN(64,return_sequences=True))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b38011",
   "metadata": {},
   "source": [
    "__Training:__ in this case we don't want to prepare the whole training dataset in advance. The main cause is that the length of every batch depends on the maximum sentence length within the batch. This leaves us two options: use custom training code as in previous seminar or use generators.\n",
    "\n",
    "Keras models have a __`model.fit_generator`__ method that accepts a python generator yielding one batch at a time. But first we need to implement such generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d159a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "\n",
    "def generate_batches(sentences,batch_size=BATCH_SIZE,max_len=None,pad=0):\n",
    "    assert isinstance(sentences,np.ndarray),\"Make sure sentences is q numpy array\"\n",
    "    \n",
    "    while True:\n",
    "        indices = np.random.permutation(np.arange(len(sentences)))\n",
    "        for start in range(0,len(indices)-1,batch_size):\n",
    "            batch_indices = indices[start:start+batch_size]\n",
    "            batch_words,batch_tags = [],[]\n",
    "            for sent in sentences[batch_indices]:\n",
    "                words,tags = zip(*sent)\n",
    "                batch_words.append(words)\n",
    "                batch_tags.append(tags)\n",
    "\n",
    "            batch_words = to_matrix(batch_words,word_to_id,max_len,pad)\n",
    "            batch_tags = to_matrix(batch_tags,tag_to_id,max_len,pad)\n",
    "\n",
    "            batch_tags_1hot = to_categorical(batch_tags,len(all_tags)).reshape(batch_tags.shape+(-1,))\n",
    "            yield batch_words,batch_tags_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2f5f5",
   "metadata": {},
   "source": [
    "__Callbacks:__ Another thing we need is to measure model performance. The tricky part is not to count accuracy after sentence ends (on padding) and making sure we count all the validation data exactly once.\n",
    "\n",
    "While it isn't impossible to persuade Keras to do all of that, we may as well write our own callback that does that.\n",
    "Keras callbacks allow you to write a custom code to be ran once every epoch or every minibatch. We'll define one via LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2691af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_accuracy(model):\n",
    "    test_words,test_tags = zip(*[zip(*sentence) for sentence in test_data])\n",
    "    test_words,test_tags = to_matrix(test_words,word_to_id),to_matrix(test_tags,tag_to_id)\n",
    "\n",
    "    #predict tag probabilities of shape [batch,time,n_tags]\n",
    "    predicted_tag_probabilities = model.predict(test_words,verbose=1)\n",
    "    predicted_tags = predicted_tag_probabilities.argmax(axis=-1)\n",
    "    \n",
    "    #compute accurary excluding padding\n",
    "    numerator = np.sum(np.logical_and((predicted_tags == test_tags),(test_words != 0)))\n",
    "    denominator = np.sum(test_words != 0)\n",
    "    return float(numerator)/denominator\n",
    "\n",
    "\n",
    "class EvaluateAccuracy(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        sys.stdout.flush()\n",
    "        print(\"\\nMeasuring validation accuracy...\")\n",
    "        acc = compute_test_accuracy(self.model)\n",
    "        print(\"\\nValidation accuracy: %.5f\\n\"%acc)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "330f962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 205s 152ms/step - loss: 0.2572\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 27ms/step\n",
      "\n",
      "Validation accuracy: 0.94026\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 205s 153ms/step - loss: 0.0583\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 27ms/step\n",
      "\n",
      "Validation accuracy: 0.94374\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 204s 152ms/step - loss: 0.0518\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 27ms/step\n",
      "\n",
      "Validation accuracy: 0.94509\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 206s 153ms/step - loss: 0.0473\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 27ms/step\n",
      "\n",
      "Validation accuracy: 0.94589\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 210s 156ms/step - loss: 0.0426\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 27ms/step\n",
      "\n",
      "Validation accuracy: 0.94524\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x285373e7940>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b894d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 12s 27ms/step\n",
      "Final accuracy: 0.94565\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"Final accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.94, \"Keras has gone on a rampage again, please contact course staff.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17669f13",
   "metadata": {},
   "source": [
    "### Going bidirectional (Задание 1)\n",
    "\n",
    "Since we're analyzing a full sequence, it's legal for us to look into future data.\n",
    "\n",
    "A simple way to achieve that is to go both directions at once, making a __bidirectional RNN__.\n",
    "\n",
    "In Keras you can achieve that both manually (using two LSTMs and Concatenate) and by using __`keras.layers.Bidirectional`__. \n",
    "\n",
    "This one works just as `TimeDistributed` we saw before: you wrap it around a recurrent layer (SimpleRNN now and LSTM/GRU later) and it actually creates two layers under the hood.\n",
    "\n",
    "Your first task is to use such a layer our POS-tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de96aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(len(all_words),50))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(64,return_sequences=True)))\n",
    "\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce505464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 408s 302ms/step - loss: 0.1877\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 24s 53ms/step\n",
      "\n",
      "Validation accuracy: 0.95649\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 405s 301ms/step - loss: 0.0425\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 49ms/step\n",
      "\n",
      "Validation accuracy: 0.96094\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 400s 297ms/step - loss: 0.0352\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 48ms/step\n",
      "\n",
      "Validation accuracy: 0.96288\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 393s 292ms/step - loss: 0.0297\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 48ms/step\n",
      "\n",
      "Validation accuracy: 0.96315\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 393s 293ms/step - loss: 0.0249\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 48ms/step\n",
      "\n",
      "Validation accuracy: 0.96148\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2868ec04490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e5759",
   "metadata": {},
   "source": [
    "Measure final accuracy on the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53c48fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 23s 50ms/step\n",
      "\n",
      "Final accuracy: 0.96148\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.96, \"Bidirectional RNNs are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b1165",
   "metadata": {},
   "source": [
    "### Task I: Structured loss functions (more bonus points) (Задание 2)\n",
    "\n",
    "Since we're tagging the whole sequence at once, we might as well train our network to do so. Remember linear CRF from the lecture? You can also use it as a loss function for your RNN\n",
    "\n",
    "\n",
    "  * There's more than one way to do so, but we'd recommend starting with [Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n",
    "  * You can plug CRF as a loss function and still train by backprop. There's even some neat tensorflow [implementation](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/CRF) for you.\n",
    "  * Alternatively, you can condition your model on previous tags (make it autoregressive) and perform __beam search__ over that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3efa24",
   "metadata": {},
   "source": [
    "Так как задание необязательное, я его пропускаю. \n",
    "\n",
    "Лучшая моя попытка подключить CRF выглядит так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ba3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input([None], dtype='int32')\n",
    "x = tf.keras.layers.Embedding(len(all_words), 50)(inputs)\n",
    "x = tf.keras.layers.SimpleRNN(64, return_sequences=True)(x)\n",
    "x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(all_tags), activation='softmax'))(x)  \n",
    "\n",
    "crf = tfa.CRF(len(all_tags))\n",
    "\n",
    "decoded_sequence, potentials, sequence_length, chain_kernel = crf(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=potentials, name=\"our_first_model\")\n",
    "\n",
    "model.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46880af7",
   "metadata": {},
   "source": [
    "Слой добавлен, но с моделью явно что-то не так. Скорее всего я либо неправильно передаю функцию в add_loss, либо outputs при создании модели должен быть другой. В документации tfa.CRF нет нормального примера (либо я не смогла его применить).\n",
    "\n",
    "Буду благодарна за ссылку, где можно посмотреть нормальный пример, как правильно настраивать tfa.CRF (или хотя  бы куда копать)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f291d41",
   "metadata": {},
   "source": [
    "#### Some tips\n",
    "Here there are a few more tips on how to improve training that are a bit trickier to impliment. We strongly suggest that you try them _after_ you've got a good initial model.\n",
    "* __Use pre-trained embeddings__: you can use pre-trained weights from [there](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) to kickstart your Embedding layer.\n",
    "  * Embedding layer has a matrix W (layer.W) which contains word embeddings for each word in the dictionary. You can just overwrite them with tf.assign.\n",
    "  * When using pre-trained embeddings, pay attention to the fact that model's dictionary is different from your own.\n",
    "  * You may want to switch trainable=False for embedding layer in first few epochs as in regular fine-tuning.  \n",
    "* __Go beyond SimpleRNN__: there's `keras.layers.LSTM` and `keras.layers.GRU`\n",
    "  * If you want to use a custom recurrent Cell, read [this](https://keras.io/layers/recurrent/#rnn)\n",
    "  * You can also use 1D Convolutions (`keras.layers.Conv1D`). They are often as good as recurrent layers but with less overfitting.\n",
    "* __Stack more layers__: if there is a common motif to this course it's about stacking layers\n",
    "  * You can just add recurrent and 1dconv layers on top of one another and keras will understand it\n",
    "  * Just remember that bigger networks may need more epochs to train\n",
    "* __Regularization__: you can apply dropouts as usual but also in an RNN-specific way\n",
    "  * `keras.layers.Dropout` works inbetween RNN layers\n",
    "  * Recurrent layers also have `recurrent_dropout` parameter\n",
    "* __Gradient clipping__: If your training isn't as stable as you'd like, set `clipnorm` in your optimizer.\n",
    "  * Which is to say, it's a good idea to watch over your loss curve at each minibatch. Try tensorboard callback or something similar.\n",
    "* __Word Dropout__: tl;dr randomly replace words with UNK during training. \n",
    "  * This can also simulate increased amount of unknown words in the test set\n",
    "* __Larger vocabulary__: You can obtain greater performance by expanding your model's input dictionary from 5000 to up to every single word!\n",
    "  * Just make sure your model doesn't overfit due to so many parameters.\n",
    "  * Combined with regularizers or pre-trained word-vectors this could be really good cuz right now our model is blind to >5% of words.  \n",
    "* __More efficient batching__: right now TF spends a lot of time iterating over \"0\"s\n",
    "  * This happens because batch is always padded to the length of a longest sentence\n",
    "  * You can speed things up by pre-generating batches of similar lengths and feeding it with randomly chosen pre-generated batch.\n",
    "  * This technically breaks the i.i.d. assumption, but it works unless you come up with some insane rnn architectures.\n",
    "* __The most important advice__: don't cram in everything at once!\n",
    "  * If you stuff in a lot of modiffications, some of them almost inevitably gonna be detrimental and you'll never know which of them are.\n",
    "  * Try to instead go in small iterations and record experiment results to guide further search.\n",
    "    \n",
    "Good hunting!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03167065",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Для чистоты эксперимента пока без Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "c8503314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(len(all_words),50))\n",
    "model.add(tf.keras.layers.LSTM(64,return_sequences=True))\n",
    "\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "35020e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 14s 8ms/step - loss: 0.3349\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.93931\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 10s 8ms/step - loss: 0.0612:\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.94527\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 11s 8ms/step - loss: 0.0531\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.94729\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 11s 8ms/step - loss: 0.0490\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.94884\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 11s 8ms/step - loss: 0.0459\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.94913\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28531c2ad90>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e797cf",
   "metadata": {},
   "source": [
    "LSTM показала результат чуть лучше, чем RNN, но обучилась намного быстрее"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7c3d4",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "fddb40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(len(all_words),50))\n",
    "model.add(tf.keras.layers.GRU(64, return_sequences=True))\n",
    "\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "8e95ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 12s 8ms/step - loss: 0.2362\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.93931\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 10s 7ms/step - loss: 0.0582\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.94583\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 10s 8ms/step - loss: 0.0517\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 1s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.94763\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 10s 7ms/step - loss: 0.0482\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.94891\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 10s 7ms/step - loss: 0.0449\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 1s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.94952\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2869f3ad6d0>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9bf3f",
   "metadata": {},
   "source": [
    "GRU сеть показала такой же результат, как LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f227ca",
   "metadata": {},
   "source": [
    "### LSTM / trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09dfa006",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(len(all_words),50, trainable=True))\n",
    "model.add(tf.keras.layers.LSTM(64,return_sequences=True))\n",
    "\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c28f60e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 64)          29440     \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, None, 14)          910       \n",
      "=================================================================\n",
      "Total params: 530,450\n",
      "Trainable params: 530,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "d8690d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 12s 8ms/step - loss: 0.3354\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.93959\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 10s 7ms/step - loss: 0.0612\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.94468\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 10s 8ms/step - loss: 0.0531\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.94765\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 10s 8ms/step - loss: 0.0486\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.94842\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 11s 8ms/step - loss: 0.0456\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 2s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.94903\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x286c4647b50>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a65fb",
   "metadata": {},
   "source": [
    "Результат даже немного ухудшился.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a2238b",
   "metadata": {},
   "source": [
    "### LSTM / trainable=True / Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db44fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(len(all_words),50, trainable=True))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3582144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 19s 12ms/step - loss: 0.2693\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 4s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.95417\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 18s 13ms/step - loss: 0.0459\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96013\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 18s 13ms/step - loss: 0.0377\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 8ms/step\n",
      "\n",
      "Validation accuracy: 0.96309\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 17s 13ms/step - loss: 0.0330\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96405\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 17s 12ms/step - loss: 0.0288\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 6ms/step\n",
      "\n",
      "Validation accuracy: 0.96547\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28af93de8e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653abd9c",
   "metadata": {},
   "source": [
    "Bidirectional по прежнему работает"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88358bea",
   "metadata": {},
   "source": [
    "### LSTM / trainable=True / Bidirectional / pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1db88818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e059451",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for sentence in train_data:\n",
    "    X_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0])\n",
    "        \n",
    "    X.append(X_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0f1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f0994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Elena/gensim-data/word2vec-google-news-300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f7ca09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE  = 300\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "word2id = word_tokenizer.word_index\n",
    "\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f703e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43851, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bdde0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, trainable=True, weights=[embedding_weights]))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0fc74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1343/1343 [==============================] - 33s 23ms/step - loss: 0.0182\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96536\n",
      "\n",
      "Epoch 2/10\n",
      "1343/1343 [==============================] - 30s 23ms/step - loss: 0.0147\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96432\n",
      "\n",
      "Epoch 3/10\n",
      "1343/1343 [==============================] - 30s 23ms/step - loss: 0.0114\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96394\n",
      "\n",
      "Epoch 4/10\n",
      "1343/1343 [==============================] - 30s 22ms/step - loss: 0.0088\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96208\n",
      "\n",
      "Epoch 5/10\n",
      "1343/1343 [==============================] - 30s 23ms/step - loss: 0.0068\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96179\n",
      "\n",
      "Epoch 6/10\n",
      "1343/1343 [==============================] - 30s 23ms/step - loss: 0.0051\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96200\n",
      "\n",
      "Epoch 7/10\n",
      "1343/1343 [==============================] - 30s 23ms/step - loss: 0.0040\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 6ms/step\n",
      "\n",
      "Validation accuracy: 0.96058\n",
      "\n",
      "Epoch 8/10\n",
      "1343/1343 [==============================] - 30s 22ms/step - loss: 0.0032\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96016\n",
      "\n",
      "Epoch 9/10\n",
      "1343/1343 [==============================] - 30s 23ms/step - loss: 0.0025\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96050\n",
      "\n",
      "Epoch 10/10\n",
      "1343/1343 [==============================] - 30s 23ms/step - loss: 0.0021\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 7ms/step\n",
      "\n",
      "Validation accuracy: 0.96032\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28bec3c7550>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b74c3",
   "metadata": {},
   "source": [
    "Что-то пошло не так.. Пока оставим"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303be3c",
   "metadata": {},
   "source": [
    "### Добавлю ещё один слой Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bba6d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, trainable=True, weights=[embedding_weights]))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e11e3c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 47s 31ms/step - loss: 0.1786\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 7s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.95942\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 40s 30ms/step - loss: 0.0395 0s - loss: \n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96371\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 41s 30ms/step - loss: 0.0321\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 6s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96514\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 40s 29ms/step - loss: 0.0275\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96703\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 40s 30ms/step - loss: 0.0232\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96708\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28c0c6d64c0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc30798",
   "metadata": {},
   "source": [
    "Стало чуть лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b4a29",
   "metadata": {},
   "source": [
    "### Добавлю Dropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48ac34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, trainable=True, weights=[embedding_weights]))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b071ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 45s 30ms/step - loss: 0.1818\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 6s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.95965\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 40s 30ms/step - loss: 0.0420\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96418\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 40s 30ms/step - loss: 0.0350\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96659\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0303\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96707\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0264 0s - loss: 0.02\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96774\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28bec3c7e80>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1ae12",
   "metadata": {},
   "source": [
    "Модель не улучшилась, но и хуже не стало"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c68ca",
   "metadata": {},
   "source": [
    "### Добавлю свёртку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90ae844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input([None],dtype='int32'))\n",
    "model.add(tf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, trainable=True, weights=[embedding_weights]))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "model.add(tf.keras.layers.Conv1D(128, kernel_size=1))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "stepwise_dense = tf.keras.layers.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = tf.keras.layers.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69ada0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1343/1343 [==============================] - 44s 29ms/step - loss: 0.0285\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 6s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96771\n",
      "\n",
      "Epoch 2/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0260\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96788\n",
      "\n",
      "Epoch 3/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0237\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96705\n",
      "\n",
      "Epoch 4/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0216\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96796\n",
      "\n",
      "Epoch 5/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0199\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96726\n",
      "\n",
      "Epoch 6/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0181\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96698\n",
      "\n",
      "Epoch 7/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0167\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96707\n",
      "\n",
      "Epoch 8/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0152\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96655\n",
      "\n",
      "Epoch 9/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0142\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 12ms/step\n",
      "\n",
      "Validation accuracy: 0.96644\n",
      "\n",
      "Epoch 10/10\n",
      "1343/1343 [==============================] - 39s 29ms/step - loss: 0.0130\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 11ms/step\n",
      "\n",
      "Validation accuracy: 0.96600\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x265dd511f70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "model.fit_generator(generate_batches(train_data), len(train_data)/BATCH_SIZE, callbacks=[EvaluateAccuracy()], epochs=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c0891",
   "metadata": {},
   "source": [
    "По итогу, удалось улучшить качесвто модели, но не сильно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56678f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
